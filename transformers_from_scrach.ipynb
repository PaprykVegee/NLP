{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer: BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "configuration: BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "model_cpk = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_cpk)\n",
    "config = AutoConfig.from_pretrained(model_cpk)\n",
    "\n",
    "print(f'tokenizer: {tokenizer}')\n",
    "print(f\"configuration: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs IDs: tensor([[ 2051, 10029,  2066,  2019,  8612]])\n",
      "inputs embedding size: torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "text = 'time flies like an arrow'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
    "print(f\"inputs IDs: {inputs.input_ids}\")\n",
    "\n",
    "embedded_layer = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "inputs_emb = embedded_layer(inputs.input_ids)\n",
    "print(f'inputs embedding size: {inputs_emb.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function calculate an attention value using dot product method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "def scaled_dot_preduct_attention(query, key, values) -> torch.Tensor:\n",
    "    size_k = query.size(-1)\n",
    "    score = torch.bmm(query, key.transpose(1, 2))/sqrt(size_k)\n",
    "    weights = F.softmax(score, dim=-1)\n",
    "\n",
    "    return torch.bmm(weights, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attension head - one of the basic componet of attension mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embedded_dim, head_num) -> None:\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embedded_dim, head_num)\n",
    "        self.k = nn.Linear(embedded_dim, head_num)\n",
    "        self.v = nn.Linear(embedded_dim, head_num)\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "        atten_out = scaled_dot_preduct_attention(self.q(hidden_state),\n",
    "                                                 self.k(hidden_state),\n",
    "                                                 self.v(hidden_state))\n",
    "        return atten_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi head attention - some AttentionHead connected to one. Typice BERT model have 12 head|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        embedding_dim = config.hidden_size\n",
    "        num_head = config.num_attention_heads\n",
    "        head_dim = embedding_dim//num_head\n",
    "        \n",
    "        self.heads = nn.ModuleList([AttentionHead(embedding_dim, num_head) for _ in range(head_dim)])\n",
    "        self.output_liner = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.output_liner(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "MultiHead = MultiHeadAttention(config)\n",
    "MultiHead(inputs_emb).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size,\n",
    "                                  config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size,\n",
    "                                  config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "feed_forward = FeedForward(config)\n",
    "feed_forward(inputs_emb).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement pre post layer normalization in transformer encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_normalization_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_normalization_2 = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_normalization_1(x)\n",
    "        x = x + self.attention(x)\n",
    "        x = self.layer_normalization_2(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        x = self.layer_normalization_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "Transformer_encoder = TransformerEncoderLayer(config)\n",
    "Transformer_encoder(inputs_emb).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeding - its make embeding on enkoded position of token in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.embeding_token = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.embeding_position = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Create positions od IDs for input sequence\n",
    "        seq_len = x.size(1)\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(dim=0)\n",
    "\n",
    "        # Create token and position embeded\n",
    "        token_emb = self.embeding_token(x)\n",
    "        position_emb = self.embeding_position(position_ids)\n",
    "\n",
    "        embedings = token_emb + position_emb\n",
    "        embedings = self.layer_norm(embedings)\n",
    "\n",
    "        embedings = self.dropout(embedings)\n",
    "\n",
    "        return embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "emb = Embedding(config)\n",
    "emb(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full tansformer encoder using config.num_hidden_layers (12) typice for BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeded = Embedding(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embeded(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "Transformer_enc = TransformerEncoder(config)\n",
    "Transformer_enc(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clf head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.clf = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Wywołanie metody forward encoder'a\n",
    "        x = self.encoder(x)\n",
    "        # Pobranie wektora dla [CLS] (pierwszy token)\n",
    "        x = x[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.clf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels = 3\n",
    "\n",
    "Transformer_clf = TransformerForSequenceClassification(config)\n",
    "Transformer_clf(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer with Decoder layer\n",
    "\n",
    "Only modify relative to encoder layer is that we use mask (triangular matrix of weights attention to protect model to cheat and copy output. No predict|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, values, mask=None):\n",
    "    dim_k = query.size(-1)\n",
    "    score = torch.bmm(query, key.transpose(1, 2))/sqrt(dim_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        seq_len = query.size(-2)\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "\n",
    "        score = score.masked_fill(mask==0, -float('inf'))\n",
    "\n",
    "    weight = F.Softmax(score)\n",
    "\n",
    "    return torch.bmm(values, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attension head similar to encoder Attension head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHeadDecoder(nn.Module):\n",
    "    def __init__(self, embedded_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embedded_dim, head_dim)\n",
    "        self.k = nn.Linear(embedded_dim, head_dim)\n",
    "        self.v = nn.Linear(embedded_dim, head_dim)\n",
    "\n",
    "    def forward(self, query, key, value) -> torch.Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value), mask=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embedding_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embedding_dim // num_heads\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHeadDecoder(embedding_dim, head_dim) for _ in range(num_heads)])\n",
    "        self.linear_output = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, query, key, value) -> torch.Tensor:\n",
    "        x = torch.cat([head(query, key, value) for head in self.heads], dim=-1)\n",
    "        return self.linear_output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferent on decoder and encoder layer is that the decoder have cross attension. Thats mean we use encoder output to calulate a attension (secound attenson cause first is calcualte the same in encoder layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_3 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttentionDecoder(config)\n",
    "        self.cross_attention = MultiHeadAttentionDecoder(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x, encoder_out):\n",
    "        attn_out = self.attention(self.layer_norm_1(x), x, x)\n",
    "        x = x + attn_out\n",
    "        cross_attn_out = self.cross_attention(self.layer_norm_2(x), encoder_out, encoder_out)\n",
    "        x = x + cross_attn_out\n",
    "        ff_out = self.feed_forward(self.layer_norm_3(x))\n",
    "        x = x + ff_out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output)\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.decoder = TransformerDecoder(config)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        encoder_output = self.encoder(src)\n",
    "        decoder_output = self.decoder(tgt, encoder_output)\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(config)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.clf = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        transformer_output = self.transformer(src, tgt)\n",
    "        x = transformer_output[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.clf(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
